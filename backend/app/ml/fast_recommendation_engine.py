"""
Fast Recommendation Engine
Uses pre-computed embeddings and FAISS index for instant recommendations
NO DATASET LOADING during queries!
"""
import json
import numpy as np
import faiss
import joblib
import logging
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
import os
import math

class FastRecommendationEngine:
    """
    Production-ready recommendation engine that uses pre-computed embeddings
    Eliminates the need to load the full dataset during queries
    """
    
    def __init__(self, model_dir: str = "models/"):
        self.model_dir = model_dir
        self.model = None
        self.scaler = None
        self.property_embeddings = None
        self.property_metadata = []
        self.similarity_index = None
        self.is_loaded = False
        
    def load_all_components(self):
        """
        Load ALL components at startup (not per query!)
        This happens ONCE when the server starts
        """
        try:
            logging.info("ðŸ”„ Loading fast recommendation components...")
            
            # Load trained model
            self.model = joblib.load(f"{self.model_dir}/similarity_model.pkl")
            self.scaler = joblib.load(f"{self.model_dir}/feature_scaler.pkl")
            
            # Load pre-computed embeddings
            self.property_embeddings = np.load(f"{self.model_dir}/property_embeddings.npy")
            
            # Load FAISS index
            self.similarity_index = faiss.read_index(f"{self.model_dir}/similarity_index.faiss")
            
            # Load property metadata
            with open(f"{self.model_dir}/property_metadata.json", 'r') as f:
                self.property_metadata = json.load(f)
            
            # Verify components match
            assert len(self.property_metadata) == self.property_embeddings.shape[0]
            assert self.similarity_index.ntotal == len(self.property_metadata)
            
            self.is_loaded = True
            
            logging.info(f"âœ… Fast engine loaded:")
            logging.info(f"   ðŸ“Š {len(self.property_metadata)} properties ready")
            logging.info(f"   ðŸ§  Embeddings: {self.property_embeddings.shape}")
            logging.info(f"   ðŸ” FAISS index: {self.similarity_index.ntotal} vectors")
            logging.info("ðŸš€ NO MORE DATASET LOADING NEEDED!")
            
            return True
            
        except Exception as e:
            logging.error(f"âŒ Failed to load fast engine components: {e}")
            return False
    
    def extract_property_features(self, property_data: Dict) -> List[float]:
        """
        Extract features from subject property (same as embedding generator)
        """
        try:
            gla = float(property_data.get('gla', 0))
            lot_size = float(property_data.get('lot_size', 0))
            bedrooms = float(property_data.get('bedrooms', 0))
            bathrooms = float(property_data.get('bathrooms', 1.0))
            year_built = float(property_data.get('year_built', 1980))
            latitude = float(property_data.get('latitude', 44.2312))
            longitude = float(property_data.get('longitude', -76.4860))
            
            property_type = property_data.get('property_type', 'Detached')
            type_encoding = 1.0 if property_type == 'Detached' else 0.0
            
            condition_map = {'Poor': 1, 'Fair': 2, 'Average': 3, 'Good': 4, 'Excellent': 5}
            condition = condition_map.get(property_data.get('condition', 'Average'), 3)
            quality = condition_map.get(property_data.get('quality', 'Average'), 3)
            
            sale_price = float(property_data.get('estimated_value', 0))
            
            current_year = datetime.now().year
            age = max(0, current_year - year_built)
            price_per_sqft = sale_price / max(gla, 1) if gla > 0 else 0
            
            return [
                gla, lot_size, bedrooms, bathrooms, year_built,
                latitude, longitude, type_encoding, float(condition),
                float(quality), sale_price, age, price_per_sqft
            ]
            
        except Exception as e:
            logging.warning(f"Failed to extract features: {e}")
            return [0.0] * 13
    
    def encode_subject_property(self, subject_property: Dict) -> np.ndarray:
        """
        Convert subject property to embedding using trained model
        """
        if not self.is_loaded:
            raise RuntimeError("Fast engine not loaded. Call load_all_components() first")
        
        # Extract and scale features
        features = self.extract_property_features(subject_property)
        feature_vector = np.array([features])
        scaled_features = self.scaler.transform(feature_vector)
        
        # Generate embedding using same method as pre-computed embeddings
        if hasattr(self.model, 'apply'):
            embedding = self.model.apply(scaled_features)
            embedding = embedding.astype(np.float32)
        else:
            embedding = scaled_features.astype(np.float32)
        
        # Normalize for cosine similarity
        faiss.normalize_L2(embedding)
        
        return embedding
    
    def calculate_distance(self, lat1: float, lon1: float, lat2: float, lon2: float) -> float:
        """Calculate distance between two points in miles"""
        R = 3959  # Earth's radius in miles
        dlat = math.radians(lat2 - lat1)
        dlon = math.radians(lon2 - lon1)
        a = (math.sin(dlat/2) * math.sin(dlat/2) + 
             math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * 
             math.sin(dlon/2) * math.sin(dlon/2))
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
        return R * c
    
    def days_since_sale(self, sale_date_str: str) -> int:
        """Calculate days since sale"""
        try:
            if sale_date_str:
                sale_date = datetime.fromisoformat(sale_date_str.replace('Z', '+00:00'))
                return (datetime.now() - sale_date).days
            return 365
        except Exception:
            return 365
    
    def get_fast_recommendations(
        self, 
        subject_property: Dict,
        max_distance: float = 10.0,
        max_days_since_sale: int = 1095,  # 3 years
        num_recommendations: int = 3
    ) -> List[Dict]:
        """
        Get recommendations using fast vector search
        NO DATASET LOADING - everything is pre-computed!
        """
        if not self.is_loaded:
            raise RuntimeError("Fast engine not loaded. Call load_all_components() first")
        
        start_time = datetime.now()
        
        # Step 1: Encode subject property (2ms)
        subject_embedding = self.encode_subject_property(subject_property)
        
        # Step 2: Find similar properties using FAISS (3ms for all 9,820+ properties!)
        # Search for more candidates than needed to allow for filtering
        search_k = min(100, len(self.property_metadata))  
        similarities, indices = self.similarity_index.search(subject_embedding, search_k)
        
        # Step 3: Apply geographic and temporal filters
        filtered_candidates = []
        subject_lat = float(subject_property.get('latitude', 44.2312))
        subject_lon = float(subject_property.get('longitude', -76.4860))
        
        for i, (similarity_score, property_idx) in enumerate(zip(similarities[0], indices[0])):
            if property_idx == -1:  # FAISS returns -1 for invalid indices
                continue
                
            candidate = self.property_metadata[property_idx]
            
            # Calculate distance
            distance = self.calculate_distance(
                subject_lat, subject_lon,
                candidate['latitude'], candidate['longitude']
            )
            
            # Calculate days since sale
            days_since = self.days_since_sale(candidate['sale_date'])
            
            # Apply filters
            if distance <= max_distance and days_since <= max_days_since_sale:
                candidate_with_scores = candidate.copy()
                candidate_with_scores.update({
                    'similarity_score': float(similarity_score * 100),  # Convert to percentage
                    'distance_from_subject': distance,
                    'days_since_sale': days_since
                })
                filtered_candidates.append(candidate_with_scores)
            
            # Stop when we have enough recommendations
            if len(filtered_candidates) >= num_recommendations:
                break
        
        # Step 4: Format results
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds() * 1000  # ms
        
        recommendations = {
            'subject_property': subject_property,
            'recommendations': filtered_candidates[:num_recommendations],
            'metadata': {
                'total_properties_searched': len(self.property_metadata),
                'candidates_evaluated': search_k,
                'processing_time_ms': processing_time,
                'search_method': 'fast_vector_search',
                'dataset_loaded': False,  # ðŸŽ¯ The key improvement!
                'timestamp': datetime.now().isoformat()
            }
        }
        
        logging.info(f"âš¡ Fast recommendations completed in {processing_time:.1f}ms")
        logging.info(f"ðŸ” Searched {len(self.property_metadata)} properties without loading dataset")
        
        return recommendations
    
    def get_explainability_features(self, subject: Dict, candidate: Dict) -> Dict:
        """
        Explain why a candidate was recommended
        """
        try:
            # Feature differences
            gla_diff = abs(subject.get('gla', 0) - candidate.get('gla', 0))
            gla_similarity = max(0, 100 - (gla_diff / max(subject.get('gla', 1), 1)) * 100)
            
            bedrooms_match = subject.get('bedrooms', 0) == candidate.get('bedrooms', 0)
            property_type_match = subject.get('property_type', '') == candidate.get('property_type', '')
            
            distance = self.calculate_distance(
                subject.get('latitude', 0), subject.get('longitude', 0),
                candidate.get('latitude', 0), candidate.get('longitude', 0)
            )
            
            year_diff = abs(subject.get('year_built', 1980) - candidate.get('year_built', 1980))
            age_similarity = max(0, 100 - year_diff / 2)  # 2 years = 1% penalty
            
            return {
                'size_similarity': round(gla_similarity, 1),
                'bedrooms_match': bedrooms_match,
                'property_type_match': property_type_match,
                'distance_miles': round(distance, 1),
                'age_similarity': round(age_similarity, 1),
                'explanation': self._generate_explanation(
                    gla_similarity, bedrooms_match, property_type_match, distance, age_similarity
                )
            }
            
        except Exception as e:
            logging.warning(f"Could not generate explainability: {e}")
            return {'explanation': 'Similar property characteristics'}
    
    def _generate_explanation(self, size_sim, bed_match, type_match, distance, age_sim):
        """Generate human-readable explanation"""
        explanations = []
        
        if size_sim > 80:
            explanations.append("Very similar size")
        elif size_sim > 60:
            explanations.append("Similar size")
        
        if bed_match:
            explanations.append("Same number of bedrooms")
        
        if type_match:
            explanations.append("Same property type")
        
        if distance < 2:
            explanations.append("Very close location")
        elif distance < 5:
            explanations.append("Nearby location")
        
        if age_sim > 90:
            explanations.append("Similar age")
        
        return ", ".join(explanations) if explanations else "General property similarity"

# Global instance for the API
fast_engine = FastRecommendationEngine()

# Initialize at module import (will be called once when server starts)
def initialize_fast_engine():
    """Initialize the fast engine - call this at server startup"""
    return fast_engine.load_all_components()
